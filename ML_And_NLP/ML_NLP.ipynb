{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import bigrams\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STORY = \"story\"\n",
    "ASK_HN = \"ask_hn\"\n",
    "SHOW_HN = \"show_hn\"\n",
    "POLL = \"poll\"\n",
    "\n",
    "def get_pos_tag(word) :\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_and_store(sentence, vocabulary, post_type_vocabulary) :\n",
    "    lemmatized_words = []\n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        if word not in string.punctuation :\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "            add_to_vocabulary(lemmatized_word, vocabulary)\n",
    "            add_to_vocabulary(lemmatized_word, post_type_vocabulary)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def add_to_vocabulary(word, vocabulary) :\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] += 1\n",
    "    else :\n",
    "        vocabulary[word] = 1\n",
    "\n",
    "\n",
    "def get_lemmatized_words(sentence) :\n",
    "    lemmatized_words = []\n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        if word not in string.punctuation :\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def create_vocabulary(posts, vocabulary, post_type_vocabulary, bigram_vocabulary, post_type) :\n",
    "    for index, post in posts.iterrows():\n",
    "        if index > 1000:\n",
    "            break\n",
    "        cleaned_words = lemmatize_and_store(post.Title, vocabulary, post_type_vocabulary)\n",
    "        string_bigrams = bigrams(cleaned_words)\n",
    "        for gram in string_bigrams: \n",
    "            add_to_bigram_vocabulary(gram, bigram_vocabulary, post_type)\n",
    "\n",
    "\n",
    "def add_to_bigram_vocabulary(word, bigram_vocabulary, post_type) :\n",
    "    if word in bigram_vocabulary :\n",
    "        bigram_vocabulary[word][0] += 1\n",
    "        if post_type in bigram_vocabulary[word][1] :\n",
    "            bigram_vocabulary[word][1][post_type] += 1\n",
    "        else :\n",
    "            bigram_vocabulary[word][1][post_type] = 1\n",
    "    else :\n",
    "        post_type_dict = dict()\n",
    "        post_type_dict[post_type] = 1\n",
    "        bigram_vocabulary[word] = [1, post_type_dict]\n",
    "\n",
    "\n",
    "def calculate_bigram_prob(bigram_vocabulary, vocabulary, bigram_probability, delta):\n",
    "    for gram in bigram_vocabulary:\n",
    "        prob = (bigram_vocabulary[gram][0] + DELTA) / (vocabulary[gram[0]] + (len(vocabulary) * delta))\n",
    "        bigram_probability[gram] = prob\n",
    "\n",
    "\n",
    "def calculate_conditional_prob(values, word, post_type_vocab, post_type_total_words, vocabulary_size, delta):\n",
    "    word_count = 0\n",
    "    if word in  post_type_vocab:\n",
    "        word_count = post_type_vocab[word]\n",
    "    conditional_prob = (word_count + delta) / (post_type_total_words + (vocabulary_size * delta))\n",
    "    values.append(word_count)\n",
    "    #values.append(conditional_prob)\n",
    "    values.append(math.log10(conditional_prob))\n",
    "\n",
    "\n",
    "def create_line(line_no, title, values):\n",
    "    line = str(line_no) + \"  \"  + title\n",
    "    for value in values :\n",
    "        line += \"  \" + str(value)\n",
    "    line += \"\\n\"\n",
    "    return line\n",
    "\n",
    "\n",
    "def calculate_score(words, training_model, post_type_probability, index):\n",
    "    post_type_score = math.log10(post_type_probability)\n",
    "    for word in words:\n",
    "        if word in training_model:\n",
    "            post_type_score += training_model[word][index]\n",
    "            #post_type_score += math.log10(training_model[word][index])\n",
    "    return post_type_score\n",
    "    \n",
    "def predict_post_type(story_score, ask_score, show_score, poll_score):\n",
    "    scores = [story_score, ask_score, show_score, poll_score]\n",
    "    max_index = scores.index(max(scores))\n",
    "    if max_index == 0:\n",
    "        return STORY\n",
    "    elif max_index == 1:\n",
    "        return ASK_HN\n",
    "    elif max_index == 2:\n",
    "        return SHOW_HN\n",
    "    else:\n",
    "        return POLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Post:  2492829\n"
     ]
    }
   ],
   "source": [
    "csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "data_2018 = csvdf[(csvdf[\"Created At\"] >= \"2018-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2018-12-31 23:59:59\")]\n",
    "\n",
    "total_post = data_2018.size\n",
    "print(\"Total Post: \", total_post)\n",
    "\n",
    "data_2018 = data_2018.groupby(\"Post Type\")\n",
    "\n",
    "story_posts = data_2018.get_group(STORY)\n",
    "ask_posts = data_2018.get_group(ASK_HN)\n",
    "show_posts = data_2018.get_group(SHOW_HN)\n",
    "poll_posts = data_2018.get_group(POLL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Done..\n",
      "Task 1 Done..\n"
     ]
    }
   ],
   "source": [
    "DELTA = 0.5\n",
    "\n",
    "# Task 0 Start - Create Vocabulary\n",
    "vocabulary = dict()\n",
    "story_post_vocabulary = dict()\n",
    "ask_post_vocabulary = dict()\n",
    "show_post_vocabulary = dict()\n",
    "poll_post_vocabulary = dict()\n",
    "bigram_vocabulary = dict()\n",
    "bigram_probability = dict()\n",
    "\n",
    "create_vocabulary(story_posts, vocabulary, story_post_vocabulary, bigram_vocabulary, STORY)\n",
    "create_vocabulary(ask_posts, vocabulary, ask_post_vocabulary, bigram_vocabulary, ASK_HN)\n",
    "create_vocabulary(show_posts, vocabulary, show_post_vocabulary, bigram_vocabulary, SHOW_HN)\n",
    "create_vocabulary(poll_posts, vocabulary, poll_post_vocabulary, bigram_vocabulary, POLL)\n",
    "\n",
    "# for gram in bigram_vocabulary:\n",
    "#     print(gram, \": \", bigram_vocabulary[gram][0])\n",
    "\n",
    "# calculate_bigram_prob(bigram_vocabulary, vocabulary, bigram_probability, DELTA)\n",
    "\n",
    "# bigram_probability = collections.OrderedDict(sorted(bigram_probability.items(), key=lambda kv:kv[1], reverse=True))\n",
    "# for gram in bigram_probability:\n",
    "#     print(gram, \": \", bigram_probability[gram])\n",
    "\n",
    "vocabulary_file = open(\"vocabulary.txt\", \"w\")\n",
    "for word in vocabulary.keys():\n",
    "    vocabulary_file.write(word+\"\\n\")\n",
    "vocabulary_file.close()\n",
    "\n",
    "print(\"Vocabulary Done..\")\n",
    "# Task 0 End\n",
    "\n",
    "\n",
    "# Task 1 Start - Build the model\n",
    "training_model = dict()\n",
    "\n",
    "story_post_total_words = sum(story_post_vocabulary.values())\n",
    "ask_post_total_words = sum(ask_post_vocabulary.values())\n",
    "show_post_total_words = sum(show_post_vocabulary.values())\n",
    "poll_post_total_words = sum(poll_post_vocabulary.values())\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "story_probability = story_posts.size / total_post\n",
    "ask_probability = ask_posts.size / total_post\n",
    "show_probability = show_posts.size / total_post\n",
    "poll_probability = poll_posts.size / total_post\n",
    "\n",
    "# Sort vocabulary alphabetically\n",
    "vocabulary = collections.OrderedDict(sorted(vocabulary.items(), key=lambda kv:kv[0]))\n",
    "\n",
    "model_file = open(\"model-2018.txt\", \"w\")\n",
    "line_no = 1\n",
    "\n",
    "for word in vocabulary.keys():\n",
    "    values = []\n",
    "    calculate_conditional_prob(values, word, story_post_vocabulary, story_post_total_words, vocabulary_size, DELTA)\n",
    "    calculate_conditional_prob(values, word, ask_post_vocabulary, ask_post_total_words, vocabulary_size, DELTA)\n",
    "    calculate_conditional_prob(values, word, show_post_vocabulary, show_post_total_words, vocabulary_size, DELTA)\n",
    "    calculate_conditional_prob(values, word, poll_post_vocabulary, poll_post_total_words, vocabulary_size, DELTA)\n",
    "    training_model[word] = values\n",
    "    \n",
    "    model_file.write(create_line(line_no, word, values))\n",
    "    line_no += 1\n",
    "    \n",
    "model_file.close()\n",
    "# Task 1 End\n",
    "\n",
    "print(\"Task 1 Done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Done..\n"
     ]
    }
   ],
   "source": [
    "# Task 2 Start - Test dataset\n",
    "data_2019 = csvdf[(csvdf[\"Created At\"] >= \"2019-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2019-12-31 23:59:59\")]\n",
    "\n",
    "baseline_result = open(\"baseline-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "line_no = 1\n",
    "\n",
    "for index, post in data_2019.iterrows():\n",
    "    cleaned_words = get_lemmatized_words(post.Title)\n",
    "    story_score = calculate_score(cleaned_words, training_model, story_probability, 1)\n",
    "    ask_score = calculate_score(cleaned_words, training_model, ask_probability, 3)\n",
    "    show_score = calculate_score(cleaned_words, training_model, show_probability, 5)\n",
    "    poll_score = calculate_score(cleaned_words, training_model, poll_probability, 7)\n",
    "    predicted_post_type = predict_post_type(story_score, ask_score, show_score, poll_score)\n",
    "    original_post_type = post[\"Post Type\"]\n",
    "    output = \"right\" if original_post_type == predicted_post_type else \"wrong\"\n",
    "    \n",
    "    values = [story_score, ask_score, show_score, poll_score, predicted_post_type, output]\n",
    "#     print(post.Title)\n",
    "    baseline_result.write(create_line(line_no, post.Title+\"  \"+original_post_type, values))\n",
    "    line_no += 1\n",
    "\n",
    "baseline_result.close()\n",
    "# Task 2 End\n",
    "\n",
    "print(\"Task 2 Done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize a Sentence with the appropriate POS tag\n",
    "# sentence = \"\"\"Following mice attacks MySQL 10% HN: on UAE ASK-HR Dr. Ph.D. sagar's $300 etc. caring farmers were marching to Delhi for better living conditions. \n",
    "# Delhi police on Tuesday fired water cannons and teargas shells at protesting farmers as they tried to \n",
    "# break barricades with their cars, automobiles and tractors.\"\"\"\n",
    "\n",
    "# print(\"\\nOrignal Sentence: \")\n",
    "# print(sentence)\n",
    "\n",
    "# print(\"\\nNew Sentence: \")\n",
    "# newSentence = get_lemmatized_words(sentence)\n",
    "# print(newSentence)\n",
    "# print(\"\\nPunctuation\",string.punctuation)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# string_bigrams = bigrams(newSentence)\n",
    "# for gram in string_bigrams: \n",
    "#     print(gram)\n",
    "\n",
    "# print(\"\\nSplit:\",newSentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
