{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done..\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import bigrams\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STORY = \"story\"\n",
    "ASK_HN = \"ask_hn\"\n",
    "SHOW_HN = \"show_hn\"\n",
    "POLL = \"poll\"\n",
    "DELTA = 0.5\n",
    "\n",
    "def get_pos_tag(word) :\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_and_store(sentence, vocabulary, post_type_vocabulary) :\n",
    "    lemmatized_words = []\n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        if word not in string.punctuation :\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "            add_to_vocabulary(lemmatized_word, vocabulary)\n",
    "            add_to_vocabulary(lemmatized_word, post_type_vocabulary)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def add_to_vocabulary(word, vocabulary) :\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] += 1\n",
    "    else :\n",
    "        vocabulary[word] = 1\n",
    "\n",
    "\n",
    "def get_lemmatized_words(sentence) :\n",
    "    lemmatized_words = []\n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        if word not in string.punctuation :\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def create_vocabulary(posts, vocabulary, post_type_vocabulary, bigram_vocabulary, post_type) :\n",
    "    for index, post in posts.iterrows():\n",
    "        cleaned_words = lemmatize_and_store(post.Title, vocabulary, post_type_vocabulary)\n",
    "        string_bigrams = bigrams(cleaned_words)\n",
    "        for gram in string_bigrams: \n",
    "            add_to_bigram_vocabulary(gram, bigram_vocabulary, post_type)\n",
    "\n",
    "\n",
    "def add_to_bigram_vocabulary(word, bigram_vocabulary, post_type) :\n",
    "    if word in bigram_vocabulary :\n",
    "        bigram_vocabulary[word][0] += 1\n",
    "        if post_type in bigram_vocabulary[word][1] :\n",
    "            bigram_vocabulary[word][1][post_type] += 1\n",
    "        else :\n",
    "            bigram_vocabulary[word][1][post_type] = 1\n",
    "    else :\n",
    "        post_type_dict = dict()\n",
    "        post_type_dict[post_type] = 1\n",
    "        bigram_vocabulary[word] = [1, post_type_dict]\n",
    "\n",
    "\n",
    "def calculate_bigram_prob(bigram_vocabulary, vocabulary, bigram_probability, delta):\n",
    "    for gram in bigram_vocabulary:\n",
    "        prob = (bigram_vocabulary[gram][0] + delta) / (vocabulary[gram[0]] + (len(vocabulary) * delta))\n",
    "        bigram_probability[gram] = prob\n",
    "\n",
    "\n",
    "\n",
    "csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "data_2018 = csvdf[(csvdf[\"Created At\"] >= \"2018-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2018-12-31 23:59:59\")]\n",
    "\n",
    "data_2018 = data_2018.groupby(\"Post Type\")\n",
    "\n",
    "story_posts = data_2018.get_group(STORY)\n",
    "ask_posts = data_2018.get_group(ASK_HN)\n",
    "show_posts = data_2018.get_group(SHOW_HN)\n",
    "poll_posts = data_2018.get_group(POLL)\n",
    "\n",
    "# show_posts\n",
    "\n",
    "vocabulary = dict()\n",
    "story_post_vocabulary = dict()\n",
    "ask_post_vocabulary = dict()\n",
    "show_post_vocabulary = dict()\n",
    "poll_post_vocabulary = dict()\n",
    "bigram_vocabulary = dict()\n",
    "bigram_probability = dict()\n",
    "\n",
    "# create_vocabulary(story_posts, vocabulary, story_post_vocabulary, bigram_vocabulary, STORY)\n",
    "# create_vocabulary(ask_posts, vocabulary, ask_post_vocabulary, bigram_vocabulary, ASK_HN)\n",
    "# create_vocabulary(show_posts, vocabulary, show_post_vocabulary, bigram_vocabulary, SHOW_HN)\n",
    "create_vocabulary(poll_posts, vocabulary, poll_post_vocabulary, bigram_vocabulary, POLL)\n",
    "\n",
    "# for gram in bigram_vocabulary:\n",
    "#     print(gram, \": \", bigram_vocabulary[gram][0])\n",
    "\n",
    "calculate_bigram_prob(bigram_vocabulary, vocabulary, bigram_probability, DELTA)\n",
    "\n",
    "print(\"Done..\")\n",
    "\n",
    "bigram_probability = collections.OrderedDict(sorted(bigram_probability.items(), key=lambda kv:kv[1], reverse=True))\n",
    "# for gram in bigram_probability:\n",
    "#     print(gram, \": \", bigram_probability[gram])\n",
    "\n",
    "\n",
    "# Task 1 Build the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize a Sentence with the appropriate POS tag\n",
    "# sentence = \"\"\"Following mice attacks MySQL 10% HN: on UAE ASK-HR Dr. Ph.D. sagar's $300 etc. caring farmers were marching to Delhi for better living conditions. \n",
    "# Delhi police on Tuesday fired water cannons and teargas shells at protesting farmers as they tried to \n",
    "# break barricades with their cars, automobiles and tractors.\"\"\"\n",
    "\n",
    "# print(\"\\nOrignal Sentence: \")\n",
    "# print(sentence)\n",
    "\n",
    "# print(\"\\nNew Sentence: \")\n",
    "# newSentence = get_lemmatized_words(sentence)\n",
    "# print(newSentence)\n",
    "# print(\"\\nPunctuation\",string.punctuation)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# string_bigrams = bigrams(newSentence)\n",
    "# for gram in string_bigrams: \n",
    "#     print(gram)\n",
    "\n",
    "# print(\"\\nSplit:\",newSentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
