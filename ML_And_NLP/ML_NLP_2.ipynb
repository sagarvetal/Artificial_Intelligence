{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import bigrams\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STORY = \"story\"\n",
    "ASK_HN = \"ask_hn\"\n",
    "SHOW_HN = \"show_hn\"\n",
    "POLL = \"poll\"\n",
    "\n",
    "remove_words_file = open(\"remove_words.txt\",\"r\")\n",
    "remove_words = remove_words_file.read()\n",
    "remove_words_file.close()\n",
    "\n",
    "def get_pos_tag(word) :\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# def lemmatize_and_store(sentence, vocabulary, post_type_vocabulary) :\n",
    "#     lemmatized_words = []\n",
    "#     for word in nltk.word_tokenize(sentence.lower()) :\n",
    "#         if word not in remove_words :\n",
    "#             lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "#             lemmatized_words.append(lemmatized_word)\n",
    "#             add_to_vocabulary(lemmatized_word, vocabulary)\n",
    "#             add_to_vocabulary(lemmatized_word, post_type_vocabulary)\n",
    "#     return lemmatized_words\n",
    "\n",
    "\n",
    "def get_words_and_frequncy(sentence):\n",
    "    local_vocabulary = dict()\n",
    "    words = []\n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        word = word.replace(\"'\", \"\").strip()\n",
    "        word = word.replace(\"\\\"\", \"\").strip()\n",
    "        if len(word) == 0 :\n",
    "            continue\n",
    "        if word in remove_words :\n",
    "            continue\n",
    "        add_to_vocabulary(word, local_vocabulary, 1)\n",
    "        words.append(word)\n",
    "            \n",
    "    string_bigrams = bigrams(words.copy())\n",
    "    for gram in string_bigrams: \n",
    "        tag_1 = nltk.pos_tag([gram[0]])[0][1][0].upper()\n",
    "        tag_2 = nltk.pos_tag([gram[1]])[0][1][0].upper()\n",
    "        word = gram[0] + \" \" + gram[1]\n",
    "        if tag_1 == \"N\" and tag_2 == \"N\":\n",
    "            add_to_vocabulary(word, local_vocabulary, 1)\n",
    "            reduce_frequency(gram[0], vocabulary)\n",
    "            reduce_frequency(gram[1], vocabulary)\n",
    "    \n",
    "    return local_vocabulary\n",
    "\n",
    "\n",
    "def add_to_vocabulary(word, vocabulary, frequency) :\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] += frequency\n",
    "    else :\n",
    "        vocabulary[word] = frequency\n",
    "\n",
    "\n",
    "def reduce_frequency(word, vocabulary):\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] -= 1\n",
    "        if vocabulary[word] == 0:\n",
    "            del vocabulary[word]\n",
    "\n",
    "\n",
    "# def get_lemmatized_words(sentence) :\n",
    "#     lemmatized_words = []\n",
    "#     words = []\n",
    "#     for word in nltk.word_tokenize(sentence.lower()) :\n",
    "#         if word in remove_words :\n",
    "#             continue\n",
    "#         # Add condition for remove words\n",
    "#         words.append(word)\n",
    "    \n",
    "#     last_word = \"\"\n",
    "#     string_bigrams = bigrams(words.copy())\n",
    "#     for gram in string_bigrams: \n",
    "#         tag_1 = nltk.pos_tag([gram[0]])[0][1][0].upper()\n",
    "#         tag_2 = nltk.pos_tag([gram[1]])[0][1][0].upper()\n",
    "#         pair = gram[0] + \" \" + gram[1]\n",
    "#         if tag_1 == \"N\" or tag_2 == \"N\":\n",
    "#             words.append(pair)\n",
    "#             words.remove(gram[0])\n",
    "#             last_word = gram[1]\n",
    "    \n",
    "#     if last_word in words:\n",
    "#         words.remove(last_word)\n",
    "            \n",
    "#     for word in words :\n",
    "#         lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "#         lemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "#     return lemmatized_words\n",
    "\n",
    "\n",
    "def get_lemmatized_words(words) :\n",
    "    lemmatized_words = []\n",
    "    for word in words :\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def create_vocabulary(posts, vocabulary, post_type_vocabulary) :\n",
    "    for index, post in posts.iterrows():\n",
    "        if index > 1000:\n",
    "            break\n",
    "        local_vocabulary = get_words_and_frequncy(post.Title)\n",
    "        \n",
    "        for word, count in local_vocabulary.items():\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            add_to_vocabulary(lemmatized_word, vocabulary, count)\n",
    "            add_to_vocabulary(lemmatized_word, post_type_vocabulary, count)\n",
    "\n",
    "\n",
    "# def add_to_bigram_vocabulary(word, bigram_vocabulary, post_type) :\n",
    "#     if word in bigram_vocabulary :\n",
    "#         bigram_vocabulary[word][0] += 1\n",
    "#         if post_type in bigram_vocabulary[word][1] :\n",
    "#             bigram_vocabulary[word][1][post_type] += 1\n",
    "#         else :\n",
    "#             bigram_vocabulary[word][1][post_type] = 1\n",
    "#     else :\n",
    "#         post_type_dict = dict()\n",
    "#         post_type_dict[post_type] = 1\n",
    "#         bigram_vocabulary[word] = [1, post_type_dict]\n",
    "\n",
    "\n",
    "# def calculate_bigram_prob(bigram_vocabulary, vocabulary, bigram_probability, delta):\n",
    "#     for gram in bigram_vocabulary:\n",
    "#         prob = (bigram_vocabulary[gram][0] + DELTA) / (vocabulary[gram[0]] + (len(vocabulary) * delta))\n",
    "#         bigram_probability[gram] = prob\n",
    "\n",
    "\n",
    "def calculate_conditional_prob(values, word, post_type_vocab, post_type_total_words, vocabulary_size, delta):\n",
    "    word_count = 0\n",
    "    if word in  post_type_vocab:\n",
    "        word_count = post_type_vocab[word]\n",
    "    conditional_prob = (word_count + delta) / (post_type_total_words + (vocabulary_size * delta))\n",
    "    values.append(word_count)\n",
    "    #values.append(conditional_prob)\n",
    "    values.append(round(math.log10(conditional_prob),10))\n",
    "\n",
    "\n",
    "def create_line(line_no, title, values):\n",
    "    line = str(line_no) + \"  \"  + title\n",
    "    for value in values :\n",
    "        line += \"  \" + str(value)\n",
    "    line += \"\\n\"\n",
    "    return line\n",
    "\n",
    "\n",
    "def calculate_score(words, training_model, post_type_probability, index):\n",
    "    post_type_score = round(math.log10(post_type_probability),10)\n",
    "    for word in words:\n",
    "        if word in training_model:\n",
    "            post_type_score += training_model[word][index]\n",
    "            #post_type_score += round(math.log10(training_model[word][index]),10)\n",
    "    return post_type_score\n",
    "    \n",
    "def predict_post_type(story_score, ask_score, show_score, poll_score):\n",
    "    scores = [story_score, ask_score, show_score, poll_score]\n",
    "    max_index = scores.index(max(scores))\n",
    "    if max_index == 0:\n",
    "        return STORY\n",
    "    elif max_index == 1:\n",
    "        return ASK_HN\n",
    "    elif max_index == 2:\n",
    "        return SHOW_HN\n",
    "    else:\n",
    "        return POLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Post:  2492829\n"
     ]
    }
   ],
   "source": [
    "csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "data_2018 = csvdf[(csvdf[\"Created At\"] >= \"2018-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2018-12-31 23:59:59\")]\n",
    "\n",
    "total_post = data_2018.size\n",
    "print(\"Total Post: \", total_post)\n",
    "\n",
    "data_2018 = data_2018.groupby(\"Post Type\")\n",
    "\n",
    "story_posts = data_2018.get_group(STORY)\n",
    "ask_posts = data_2018.get_group(ASK_HN)\n",
    "show_posts = data_2018.get_group(SHOW_HN)\n",
    "poll_posts = data_2018.get_group(POLL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Done..\n",
      "Task 1 Done..\n"
     ]
    }
   ],
   "source": [
    "DELTA = 0.5\n",
    "\n",
    "# Task 0 Start - Create Vocabulary\n",
    "vocabulary = dict()\n",
    "story_post_vocabulary = dict()\n",
    "ask_post_vocabulary = dict()\n",
    "show_post_vocabulary = dict()\n",
    "poll_post_vocabulary = dict()\n",
    "# bigram_vocabulary = dict()\n",
    "# bigram_probability = dict()\n",
    "\n",
    "create_vocabulary(story_posts, vocabulary, story_post_vocabulary)\n",
    "create_vocabulary(ask_posts, vocabulary, ask_post_vocabulary)\n",
    "create_vocabulary(show_posts, vocabulary, show_post_vocabulary)\n",
    "create_vocabulary(poll_posts, vocabulary, poll_post_vocabulary)\n",
    "\n",
    "# for gram in bigram_vocabulary:\n",
    "#     print(gram, \": \", bigram_vocabulary[gram][0])\n",
    "\n",
    "# calculate_bigram_prob(bigram_vocabulary, vocabulary, bigram_probability, DELTA)\n",
    "\n",
    "# bigram_probability = collections.OrderedDict(sorted(bigram_probability.items(), key=lambda kv:kv[1], reverse=True))\n",
    "# for gram in bigram_probability:\n",
    "#     print(gram, \": \", bigram_probability[gram])\n",
    "\n",
    "vocabulary_file = open(\"vocabulary.txt\", \"w\")\n",
    "for word, count in vocabulary.items():\n",
    "    vocabulary_file.write(word+\" \"+str(count)+\"\\n\")\n",
    "vocabulary_file.close()\n",
    "\n",
    "print(\"Vocabulary Done..\")\n",
    "# Task 0 End\n",
    "\n",
    "\n",
    "# Task 1 Start - Build the model\n",
    "training_model = dict()\n",
    "\n",
    "story_post_total_words = sum(story_post_vocabulary.values())\n",
    "ask_post_total_words = sum(ask_post_vocabulary.values())\n",
    "show_post_total_words = sum(show_post_vocabulary.values())\n",
    "poll_post_total_words = sum(poll_post_vocabulary.values())\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# print(story_posts.size)\n",
    "# print(ask_posts.size)\n",
    "# print(show_posts.size)\n",
    "# print(poll_posts.size)\n",
    "story_probability = story_posts.size / total_post\n",
    "ask_probability = ask_posts.size / total_post\n",
    "show_probability = show_posts.size / total_post\n",
    "poll_probability = poll_posts.size / total_post\n",
    "\n",
    "# Sort vocabulary alphabetically\n",
    "vocabulary = collections.OrderedDict(sorted(vocabulary.items(), key=lambda kv:kv[0]))\n",
    "\n",
    "model_file = open(\"model-2018.txt\", \"w\")\n",
    "line_no = 0\n",
    "\n",
    "for word in vocabulary.keys():\n",
    "    line_no += 1\n",
    "    values = []\n",
    "    calculate_conditional_prob(values, word, story_post_vocabulary, story_post_total_words, vocabulary_size, DELTA)\n",
    "    calculate_conditional_prob(values, word, ask_post_vocabulary, ask_post_total_words, vocabulary_size, DELTA)\n",
    "    calculate_conditional_prob(values, word, show_post_vocabulary, show_post_total_words, vocabulary_size, DELTA)\n",
    "    calculate_conditional_prob(values, word, poll_post_vocabulary, poll_post_total_words, vocabulary_size, DELTA)\n",
    "    training_model[word] = values\n",
    "    \n",
    "    model_file.write(create_line(line_no, word, values))\n",
    "    \n",
    "model_file.close()\n",
    "# Task 1 End\n",
    "\n",
    "print(\"Task 1 Done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Done..\n"
     ]
    }
   ],
   "source": [
    "# Task 2 Start - Test dataset\n",
    "data_2019 = csvdf[(csvdf[\"Created At\"] >= \"2019-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2019-12-31 23:59:59\")]\n",
    "\n",
    "baseline_result = open(\"baseline-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "line_no = 0\n",
    "\n",
    "for index, post in data_2019.iterrows():\n",
    "    if line_no > 1000:\n",
    "        break\n",
    "    line_no += 1\n",
    "    words = get_words_and_frequncy(post.Title)\n",
    "    lemmatized_words = get_lemmatized_words(words.keys())\n",
    "    \n",
    "    story_score = calculate_score(lemmatized_words, training_model, story_probability, 1)\n",
    "    ask_score = calculate_score(lemmatized_words, training_model, ask_probability, 3)\n",
    "    show_score = calculate_score(lemmatized_words, training_model, show_probability, 5)\n",
    "    poll_score = calculate_score(lemmatized_words, training_model, poll_probability, 7)\n",
    "    \n",
    "    predicted_post_type = predict_post_type(story_score, ask_score, show_score, poll_score)\n",
    "    original_post_type = post[\"Post Type\"]\n",
    "    output = \"right\" if original_post_type == predicted_post_type else \"wrong\"\n",
    "    values = [original_post_type, story_score, ask_score, show_score, poll_score, predicted_post_type, output]\n",
    "    \n",
    "    baseline_result.write(create_line(line_no, post.Title, values))\n",
    "    \n",
    "\n",
    "baseline_result.close()\n",
    "# Task 2 End\n",
    "\n",
    "print(\"Task 2 Done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orignal Sentence: \n",
      "The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "\n",
      "New Sentence: \n",
      "The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "\n",
      "Punctuation !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "\n",
      "the tech\n",
      "tech that\n",
      "that was\n",
      "was fixed\n",
      "fixed in\n",
      "in 2018\n",
      "2018 and\n",
      "and the\n",
      "the tech\n",
      "tech that\n",
      "that still\n",
      "still needs\n",
      "needs fixing\n",
      "\n",
      "Split: The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "V\n",
      "v\n",
      "enjoy\n",
      "N\n",
      "10.87348\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"\"\"Following mice attacks MySQL 10% HN: on UAE ASK-HR Dr. Ph.D. sagar's $300 etc. caring farmers were marching to Delhi for better living conditions. \n",
    "Delhi police on Tuesday fired water cannons and teargas shells at protesting farmers as they tried to \n",
    "break barricades with their cars, automobiles and tractors.\"\"\"\n",
    "\n",
    "sentence = \"The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\"\n",
    "\n",
    "print(\"\\nOrignal Sentence: \")\n",
    "print(sentence)\n",
    "\n",
    "print(\"\\nNew Sentence: \")\n",
    "newSentence = sentence\n",
    "# newSentence = get_lemmatized_words(sentence)\n",
    "print(newSentence)\n",
    "print(\"\\nPunctuation\",string.punctuation)\n",
    "\n",
    "print(\"\\n\")\n",
    "string_bigrams = bigrams(nltk.word_tokenize(sentence.lower()))\n",
    "for gram in string_bigrams: \n",
    "    print(gram[0]+\" \"+gram[1])\n",
    "\n",
    "print(\"\\nSplit:\",newSentence)\n",
    "word = \"enjoyed\"\n",
    "print(nltk.pos_tag([word])[0][1][0].upper())\n",
    "print(get_pos_tag(word))\n",
    "print(lemmatizer.lemmatize(word, get_pos_tag(word)))\n",
    "# print(get_lemmatized_words(word))\n",
    "print(wordnet.NOUN.upper())\n",
    "\n",
    "print(round(10.87348434, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "['the', 'tech', 'that', 'was', 'fixed', 'in', '2018', 'and', 'the', 'tech', 'that', 'still', 'needs', 'fixing']\n",
      "['the', 'tech', 'that', 'was', 'fixed', 'in', '2018', 'and', 'the', 'tech', 'that', 'still', 'needs', 'fixing']\n",
      "('the', 'tech')\n",
      "Pair: the tech\n",
      "the\n",
      "tech\n",
      "['tech', 'that', 'was', 'fixed', 'in', '2018', 'and', 'the', 'tech', 'that', 'still', 'needs', 'fixing', 'the tech']\n",
      "('tech', 'that')\n",
      "Pair: tech that\n",
      "tech\n",
      "that\n",
      "['that', 'was', 'fixed', 'in', '2018', 'and', 'the', 'tech', 'that', 'still', 'needs', 'fixing', 'the tech', 'tech that']\n",
      "('that', 'was')\n",
      "('was', 'fixed')\n",
      "('fixed', 'in')\n",
      "('in', '2018')\n",
      "('2018', 'and')\n",
      "('and', 'the')\n",
      "('the', 'tech')\n",
      "Pair: the tech\n",
      "the\n",
      "tech\n",
      "['that', 'was', 'fixed', 'in', '2018', 'and', 'tech', 'that', 'still', 'needs', 'fixing', 'the tech', 'tech that', 'the tech']\n",
      "('tech', 'that')\n",
      "Pair: tech that\n",
      "tech\n",
      "that\n",
      "['that', 'was', 'fixed', 'in', '2018', 'and', 'that', 'still', 'needs', 'fixing', 'the tech', 'tech that', 'the tech', 'tech that']\n",
      "('that', 'still')\n",
      "('still', 'needs')\n",
      "Pair: still needs\n",
      "still\n",
      "needs\n",
      "['that', 'was', 'fixed', 'in', '2018', 'and', 'that', 'needs', 'fixing', 'the tech', 'tech that', 'the tech', 'tech that', 'still needs']\n",
      "('needs', 'fixing')\n",
      "Pair: needs fixing\n",
      "needs\n",
      "fixing\n",
      "['that', 'was', 'fixed', 'in', '2018', 'and', 'that', 'fixing', 'the tech', 'tech that', 'the tech', 'tech that', 'still needs', 'needs fixing']\n",
      "['that', 'be', 'fix', 'in', '2018', 'and', 'that', 'the tech', 'tech that', 'the tech', 'tech that', 'still needs', 'needs fixing']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\"\n",
    "lemmatized_words = []\n",
    "words = []\n",
    "print(sentence)\n",
    "print(nltk.word_tokenize(sentence.lower()))\n",
    "for word in nltk.word_tokenize(sentence.lower()) :\n",
    "    if word not in string.punctuation :\n",
    "        words.append(word)\n",
    "    # Add condition for remove words\n",
    "#     words.append(word)\n",
    "print(words)\n",
    "            \n",
    "string_bigrams = bigrams(words.copy())\n",
    "# for g in string_bigrams: \n",
    "#     print(g)\n",
    "last_word = \"\"\n",
    "for gram in string_bigrams: \n",
    "    print(gram)\n",
    "    p1 = gram[0]\n",
    "    p2 = gram[1]\n",
    "    tag_1 = nltk.pos_tag([p1])[0][1][0].upper()\n",
    "    tag_2 = nltk.pos_tag([p2])[0][1][0].upper()\n",
    "    pair = p1 + \" \" + p2\n",
    "    if tag_1 == \"N\" or tag_2 == \"N\":\n",
    "        words.append(pair)\n",
    "        print(\"Pair:\", pair)\n",
    "        print(p1)\n",
    "        print(p2)\n",
    "        words.remove(p1)\n",
    "#         words.remove(p2)\n",
    "        last_word = p2\n",
    "        print(words)\n",
    "words.remove(p2)        \n",
    "for w in words :\n",
    "    lemmatized_word = lemmatizer.lemmatize(w, get_pos_tag(w))\n",
    "    lemmatized_words.append(lemmatized_word)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
