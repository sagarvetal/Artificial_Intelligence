{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Project 2\n",
    "# Sagar Shivaji Vetal - 40071979\n",
    "# Himanshu Kohli - 40070839\n",
    "# For COMP 6721 Section FJ â€“ Fall 2019\n",
    "# -------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import bigrams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "is_stopword_exp = False\n",
    "is_wordlength_exp = False\n",
    "is_infrequentword_exp = False\n",
    "remove_words = []\n",
    "remove_symbols = []\n",
    "stop_words = []\n",
    "x_values = []\n",
    "accuracy_list = []\n",
    "\n",
    "\n",
    "class HackerNews:\n",
    "    \"\"\"\n",
    "    This class is used to hold all posts of year 2018, complete vocabulary,\n",
    "    post type wise vocabulary, post type constants, training model and pobability of each post type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This constructor initializes data members to default value.\n",
    "        \"\"\"\n",
    "        self.STORY = \"story\"\n",
    "        self.ASK_HN = \"ask_hn\"\n",
    "        self.SHOW_HN = \"show_hn\"\n",
    "        self.POLL = \"poll\"\n",
    "        self.total_post = 0\n",
    "        self.story_posts = []\n",
    "        self.ask_posts = []\n",
    "        self.show_posts = []\n",
    "        self.poll_posts = []\n",
    "        self.vocabulary = dict()\n",
    "        self.story_post_vocabulary = dict()\n",
    "        self.ask_post_vocabulary = dict()\n",
    "        self.show_post_vocabulary = dict()\n",
    "        self.poll_post_vocabulary = dict()\n",
    "        self.training_model = dict()\n",
    "        self.story_probability = 0.0\n",
    "        self.ask_probability = 0.0\n",
    "        self.show_probability = 0.0\n",
    "        self.poll_probability = 0.0\n",
    "\n",
    "\n",
    "\n",
    "def get_pos_tag(word) :\n",
    "    \"\"\"\n",
    "    This method is used to find the POS tag of given word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        Word of type string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pos_tag : string\n",
    "        It returns the POS tag of given word\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def clear_symbols(word) :\n",
    "    \"\"\"\n",
    "    This method cleans the given word by removing special characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        Word of type string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word : string\n",
    "        It returns cleaned word after removing special characters.\n",
    "    \"\"\"\n",
    "    post_types = [\"ask-hn\", \"ask_hn\", \"show-hn\", \"show_hn\"]\n",
    "    if word in post_types:\n",
    "        return word\n",
    "    for symbol in remove_symbols:\n",
    "         word = word.strip(symbol)\n",
    "    return word.strip()\n",
    "\n",
    "\n",
    "def get_words_and_frequncy(sentence):\n",
    "    \"\"\"\n",
    "    This method reads the sentence, tokenize it, remove sepcial character and words.\n",
    "    It creates bi-gram and builds the vocabulary only for given sentence. \n",
    "    It also does word filtering based on the flag of experiment type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : string\n",
    "        It is a post title from training/testing data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    local_vocabulary : dictionary\n",
    "        It returns the dictionary of vocabulary only for given sentence.\n",
    "    \"\"\"\n",
    "    local_vocabulary = dict()\n",
    "    words = []\n",
    "    \n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        word = clear_symbols(word)\n",
    "        if len(word) == 0 :\n",
    "            continue\n",
    "        if word in remove_words or word in remove_symbols:\n",
    "            continue\n",
    "        if is_stopword_exp and word in stop_words :\n",
    "            continue\n",
    "        if is_wordlength_exp and (len(word) <= 2 or len(word) >= 9):\n",
    "            continue\n",
    "        add_to_vocabulary(word, local_vocabulary, 1)\n",
    "        words.append(word)\n",
    "            \n",
    "    string_bigrams = bigrams(words.copy())\n",
    "    for gram in string_bigrams: \n",
    "        word = gram[0] + \" \" + gram[1]\n",
    "        if is_wordlength_exp and len(word) <= 2 and len(word) >= 9:\n",
    "            continue\n",
    "        tag_1 = nltk.pos_tag([gram[0]])[0][1][0].upper()\n",
    "        tag_2 = nltk.pos_tag([gram[1]])[0][1][0].upper()\n",
    "        if tag_1 == \"N\" and tag_2 == \"N\":\n",
    "            add_to_vocabulary(word, local_vocabulary, 1)\n",
    "            reduce_frequency(gram[0], local_vocabulary)\n",
    "            reduce_frequency(gram[1], local_vocabulary)\n",
    "    \n",
    "    return local_vocabulary\n",
    "\n",
    "\n",
    "def add_to_vocabulary(word, vocabulary, frequency) :\n",
    "    \"\"\"\n",
    "    This method adds the given word and its frequency to given vocabulary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        It is a word extracted from post title.\n",
    "    vocabulary : dictionary\n",
    "        It is a vocabulary created from training data\n",
    "    frequency : integer\n",
    "        It is a number of occurances of given word\n",
    "    \"\"\"\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] += frequency\n",
    "    else :\n",
    "        vocabulary[word] = frequency\n",
    "\n",
    "\n",
    "def reduce_frequency(word, vocabulary):\n",
    "    \"\"\"\n",
    "    This method reduces the frequency of given word from given vocabulary,\n",
    "    and remove the word when frequency reaches to 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        It is a word extracted from post title.\n",
    "    vocabulary : dictionary\n",
    "        It is a vocabulary created from training data\n",
    "    \"\"\"\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] -= 1\n",
    "        if vocabulary[word] == 0:\n",
    "            del vocabulary[word]\n",
    "\n",
    "\n",
    "def get_lemmatized_words(words) :\n",
    "    \"\"\"\n",
    "    This method lemmatizes the given list of words and returns it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words : string array\n",
    "        It is a list of words extracted from post title.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lemmatized_words : string array\n",
    "        It is a list of lemmatized words.\n",
    "    \"\"\"\n",
    "    lemmatized_words = []\n",
    "    for word in words :\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "        lemmatized_words.append(lemmatized_word)       \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def create_vocabulary(posts, vocabulary, post_type_vocabulary) :\n",
    "    \"\"\"\n",
    "    This method reads the post title from given posts, extracts the words from post, \n",
    "    cleans those words, also creates bi-grams, count their occurances and create the vocabulary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    posts : string array\n",
    "        It is a list of posts extracted from training data.\n",
    "    vocabulary : dictionary\n",
    "        It is a complete vocabulary created from training data.\n",
    "    post_type_vocabulary : dictionary\n",
    "        It is a vocabulary created from training data for sepcific type of post.\n",
    "    \"\"\"\n",
    "    for index, post in posts.iterrows():\n",
    "        local_vocabulary = get_words_and_frequncy(post.Title)\n",
    "        for word, count in local_vocabulary.items():\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            add_to_vocabulary(lemmatized_word, vocabulary, count)\n",
    "            add_to_vocabulary(lemmatized_word, post_type_vocabulary, count)\n",
    "\n",
    "\n",
    "def calculate_conditional_prob(values, word, post_type_vocab, post_type_total_words, vocabulary_size, delta):\n",
    "    \"\"\"\n",
    "    This method calculates the conditional probabilty of given word for given post_type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values : float array\n",
    "        It holds the frequency and conditional probabilty of given word for given post type.\n",
    "    word : string\n",
    "        It is a word of which conditional probabilty is calculated.\n",
    "    post_type_vocab : dictionary\n",
    "        It is a vocabulary created from training data for sepcific type of post.\n",
    "    post_type_total_words : integer\n",
    "        Total number of words in given post type vocabulary.\n",
    "    vocabulary_size : integer\n",
    "        Total number of words in vocabulary.\n",
    "    delta : float\n",
    "        It is a smoothing value used while calculating conditional probabilty.\n",
    "    \"\"\"\n",
    "    word_count = 0\n",
    "    if word in post_type_vocab:\n",
    "        word_count = post_type_vocab[word]\n",
    "    conditional_prob = (word_count + delta) / (post_type_total_words + (vocabulary_size * delta))\n",
    "    values.append(word_count)\n",
    "    if conditional_prob != 0:\n",
    "        conditional_prob = round(math.log10(conditional_prob),10)\n",
    "    values.append(conditional_prob)\n",
    "\n",
    "\n",
    "def create_line(line_no, title, values):\n",
    "    \"\"\"\n",
    "    This method creates a line using given parameters to write into output file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    line_no : integer\n",
    "        It is a line number in output file.\n",
    "    title : string\n",
    "        It is post title.\n",
    "    values : flaot array\n",
    "        It holds the frequency and conditional probabilty of a word for all post type.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    line : string\n",
    "        A line created using given parameters to write into output file.\n",
    "    \"\"\"\n",
    "    line = str(line_no) + \"  \"  + title\n",
    "    for value in values :\n",
    "        line += \"  \" + str(value)\n",
    "    line += \"\\n\"\n",
    "    return line\n",
    "\n",
    "\n",
    "def calculate_score(words, training_model, post_type_probability, index):\n",
    "    \"\"\"\n",
    "    This method calculates the score for each post type using given list of word\n",
    "    and probability of given post type with the use of training model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words : string array\n",
    "        It is a list of words extracted from post title of testing data.\n",
    "    training_model : dictionary\n",
    "        It is a training model created for classification.\n",
    "    post_type_probability : float\n",
    "        It is probability of given post type.\n",
    "    index : index\n",
    "        It is an index of conditional probabilty of a word in training model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    post_type_score : flaot\n",
    "        A score of title for given post type.\n",
    "    \"\"\"\n",
    "    post_type_score = round(math.log10(post_type_probability),10)\n",
    "    for word in words:\n",
    "        if word in training_model:\n",
    "            post_type_score += training_model[word][index]\n",
    "    return post_type_score\n",
    "\n",
    "\n",
    "def predict_post_type(story_score, ask_score, show_score, poll_score):\n",
    "    \"\"\"\n",
    "    This method predicts post type by comparing the score of each post type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    story_score : float\n",
    "        A score of title for story post type.\n",
    "    ask_score : float\n",
    "        A score of title for ask_hn post type.\n",
    "    show_score : float\n",
    "        A score of title for show_hn post type.\n",
    "    poll_score : float\n",
    "        A score of title for poll post type.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    post_type : string\n",
    "        A post type predicted for a post title.\n",
    "    \"\"\"\n",
    "    scores = [story_score, ask_score, show_score, poll_score]\n",
    "    max_index = scores.index(max(scores))\n",
    "    if max_index == 0:\n",
    "        return hackerNews.STORY\n",
    "    elif max_index == 1:\n",
    "        return hackerNews.ASK_HN\n",
    "    elif max_index == 2:\n",
    "        return hackerNews.SHOW_HN\n",
    "    else:\n",
    "        return hackerNews.POLL\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"\n",
    "    This method reads stopwords file and returns the list of stopwords.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stop_word_list : string array\n",
    "        A list of stopwords.\n",
    "    \"\"\"\n",
    "    stop_words_file = open(\"Stopwords.txt\",\"r\", encoding=\"ISO-8859-1\")\n",
    "    stop_word_list = stop_words_file.read().split()\n",
    "    stop_words_file.close()\n",
    "    return stop_word_list\n",
    "\n",
    "\n",
    "def get_remove_words():\n",
    "    \"\"\"\n",
    "    This method reads remove_words file and returns the list of removewords.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    remove_word_list : string array\n",
    "        A list of removewords.\n",
    "    \"\"\"\n",
    "    remove_words_file = open(\"remove_words.txt\",\"r\", encoding=\"ISO-8859-1\")\n",
    "    remove_word_list = remove_words_file.read().split()\n",
    "    remove_words_file.close()\n",
    "    return remove_word_list\n",
    "\n",
    "\n",
    "def get_remove_symbols():\n",
    "    \"\"\"\n",
    "    This method reads remove_symbols file and returns the list of special characters to be removed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    remove_word_list : string array\n",
    "        A list of special characters to be removed.\n",
    "    \"\"\"\n",
    "    remove_symbols_file = open(\"remove_symbols.txt\",\"r\", encoding=\"ISO-8859-1\")\n",
    "    remove_symbols_list = remove_symbols_file.read().split()\n",
    "    remove_symbols_file.close()\n",
    "    return remove_symbols_list\n",
    "\n",
    "\n",
    "def get_binary_list(post_type_list, post_type):\n",
    "    \"\"\"\n",
    "    This method converts the values of given list into binary values i.e 0 and 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    post_type_list : string array\n",
    "        It is a list of post type of posts.\n",
    "    post_type : string\n",
    "        It is a type of post.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    binary_list : integer array\n",
    "        A list of binary values for given list of post type.\n",
    "    \"\"\"\n",
    "    binary_list = []\n",
    "    for current_post_typ in post_type_list:\n",
    "        if post_type == current_post_typ:\n",
    "            binary_list.append(1)\n",
    "        else:\n",
    "            binary_list.append(0)\n",
    "    return binary_list\n",
    "\n",
    "\n",
    "def calculate_performance(test_post_types, predicted_post_types, post_type):\n",
    "    \"\"\"\n",
    "    This method calculates and prints the performance of classification for given post type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_post_types : string array\n",
    "        It is a list of actual post type of posts.\n",
    "    predicted_post_types : string array\n",
    "        It is a list of predicted post type of posts.\n",
    "    post_type : string\n",
    "        It is a type of post.\n",
    "    \"\"\"\n",
    "    test_set = get_binary_list(test_post_types, post_type)\n",
    "    prediction_set = get_binary_list(predicted_post_types, post_type)\n",
    "    cm = confusion_matrix(test_set, prediction_set)\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[0][0]\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_measure = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "#     print(\"\\n===============================================\")\n",
    "#     print(\"Post Type: \" , post_type.upper())\n",
    "#     print(\"Confusion Matrix: \")\n",
    "#     print(cm)\n",
    "#     print(\"Accuracy:\", accuracy)\n",
    "#     print(\"Precision:\", precision)\n",
    "#     print(\"Recall:\", recall)\n",
    "#     print(\"F1-measure:\", f1_measure)\n",
    "\n",
    "\n",
    "def check_performance(hackerNews, test_post_types, predicted_post_types):\n",
    "    \"\"\"\n",
    "    This method calculates the performance of classification for each post type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hackerNews : HackerNews object\n",
    "        An object of HackerNews class.\n",
    "    test_post_types : string array\n",
    "        It is a list of actual post type of posts.\n",
    "    predicted_post_types : string array\n",
    "        It is a list of predicted post type of posts.\n",
    "    \"\"\"\n",
    "    calculate_performance(test_post_types, predicted_post_types, hackerNews.STORY)\n",
    "    calculate_performance(test_post_types, predicted_post_types, hackerNews.ASK_HN)\n",
    "    calculate_performance(test_post_types, predicted_post_types, hackerNews.SHOW_HN)\n",
    "    calculate_performance(test_post_types, predicted_post_types, hackerNews.POLL)\n",
    "    print(\"\\n===============================================\")\n",
    "    cm = confusion_matrix(test_post_types, predicted_post_types, labels=[hackerNews.STORY, hackerNews.ASK_HN, hackerNews.SHOW_HN, hackerNews.POLL])\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(cm)\n",
    "    experiment_accuracy = accuracy_score(test_post_types, predicted_post_types)\n",
    "    accuracy_list.append(experiment_accuracy)\n",
    "    print(\"Accuracy of experiment: \", experiment_accuracy)\n",
    "    print(\"Report: \")\n",
    "    print(classification_report(test_post_types, predicted_post_types))\n",
    "    \n",
    "    \n",
    "def plot_performance(x_values, y_values, x_title):\n",
    "    \"\"\"\n",
    "    This method polts the performance of an experiment using x and y values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_values : array\n",
    "        A list number of words left in vocabulary or list delta values.\n",
    "    y_values : array\n",
    "        A list of accuracy of experiment.\n",
    "    x_title : string\n",
    "        It is a title of x axis.\n",
    "    \"\"\"\n",
    "    plt.plot(x_values, y_values) \n",
    "    plt.xlabel(x_title) \n",
    "    plt.ylabel(\"Accuracy\") \n",
    "    plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(hackerNews):\n",
    "    \"\"\"\n",
    "    This method reads the training data from given input file \n",
    "    and groups all posts by post type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hackerNews : HackerNews object\n",
    "        An object of HackerNews class.\n",
    "    \"\"\"\n",
    "    csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "    data_2018 = csvdf[(csvdf[\"Created At\"] >= \"2018-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2018-12-31 23:59:59\")]\n",
    "\n",
    "    hackerNews.total_post = data_2018.shape[0]\n",
    "    # print(\"Total Post: \", total_post)\n",
    "    data_2018 = data_2018.groupby(\"Post Type\")\n",
    "    no_of_post = 500\n",
    "    hackerNews.story_posts = data_2018.get_group(hackerNews.STORY).head(no_of_post)\n",
    "    hackerNews.ask_posts = data_2018.get_group(hackerNews.ASK_HN).head(no_of_post)\n",
    "    hackerNews.show_posts = data_2018.get_group(hackerNews.SHOW_HN).head(no_of_post)\n",
    "    hackerNews.poll_posts = data_2018.get_group(hackerNews.POLL).head(no_of_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(hackerNews):\n",
    "    \"\"\"\n",
    "    This method creates the complete vocabulary and writes into file.\n",
    "    Also, it creates post type wise vocabulary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hackerNews : HackerNews object\n",
    "        An object of HackerNews class.\n",
    "    \"\"\" \n",
    "    print(\"\\nCreating Vocabulary....\")\n",
    "    hackerNews.vocabulary.clear()\n",
    "    hackerNews.story_post_vocabulary.clear()\n",
    "    hackerNews.ask_post_vocabulary.clear()\n",
    "    hackerNews.show_post_vocabulary.clear()\n",
    "    hackerNews.poll_post_vocabulary.clear()\n",
    "    \n",
    "    create_vocabulary(hackerNews.story_posts, hackerNews.vocabulary, hackerNews.story_post_vocabulary)\n",
    "    create_vocabulary(hackerNews.ask_posts, hackerNews.vocabulary, hackerNews.ask_post_vocabulary)\n",
    "    create_vocabulary(hackerNews.show_posts, hackerNews.vocabulary, hackerNews.show_post_vocabulary)\n",
    "    create_vocabulary(hackerNews.poll_posts, hackerNews.vocabulary, hackerNews.poll_post_vocabulary)\n",
    "    \n",
    "    words_tobe_removed = []\n",
    "    if is_infrequentword_exp and is_threshold_percent :\n",
    "        hackerNews.vocabulary = collections.OrderedDict(sorted(hackerNews.vocabulary.items(), key=lambda kv:kv[1], reverse=True))\n",
    "        no_of_words_to_remove = int(np.ceil(len(hackerNews.vocabulary) * (threshold / 100)))\n",
    "        for word in hackerNews.vocabulary.keys():\n",
    "            if no_of_words_to_remove == 0:\n",
    "                break;\n",
    "            else:\n",
    "                words_tobe_removed.append(word)\n",
    "                no_of_words_to_remove -= 1\n",
    "        \n",
    "    elif is_infrequentword_exp and not is_threshold_percent :\n",
    "        for word, count in hackerNews.vocabulary.items():\n",
    "            if count <= threshold:\n",
    "                words_tobe_removed.append(word)\n",
    "        \n",
    "    for word in words_tobe_removed:\n",
    "        del hackerNews.vocabulary[word]\n",
    "    \n",
    "    # Sort vocabulary alphabetically\n",
    "    hackerNews.vocabulary = collections.OrderedDict(sorted(hackerNews.vocabulary.items(), key=lambda kv:kv[0]))\n",
    "    \n",
    "    vocabulary_file = open(\"vocabulary.txt\", \"w\", encoding=\"utf-8\")\n",
    "    for word, count in hackerNews.vocabulary.items():\n",
    "        vocabulary_file.write(word+\"\\n\")\n",
    "    vocabulary_file.close()\n",
    "    \n",
    "    print(\"Vocabulary Created..!!!\")\n",
    "\n",
    "\n",
    "def create_training_model(hackerNews, model_file_name, delta): \n",
    "    \"\"\"\n",
    "    This method creates the training model and writes into file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hackerNews : HackerNews object\n",
    "        An object of HackerNews class.\n",
    "    model_file_name : string\n",
    "        A name of output file of training model.\n",
    "    delta : float\n",
    "        It is a smoothing value used while calculating conditional probabilty.\n",
    "    \"\"\" \n",
    "    print(\"\\nCreating Training Model....\")\n",
    "    story_post_total_words = sum(hackerNews.story_post_vocabulary.values())\n",
    "    ask_post_total_words = sum(hackerNews.ask_post_vocabulary.values())\n",
    "    show_post_total_words = sum(hackerNews.show_post_vocabulary.values())\n",
    "    poll_post_total_words = sum(hackerNews.poll_post_vocabulary.values())\n",
    "    vocabulary_size = len(hackerNews.vocabulary)\n",
    "    \n",
    "    hackerNews.story_probability = hackerNews.story_posts.shape[0] / hackerNews.total_post\n",
    "    hackerNews.ask_probability = hackerNews.ask_posts.shape[0] / hackerNews.total_post\n",
    "    hackerNews.show_probability = hackerNews.show_posts.shape[0] / hackerNews.total_post\n",
    "    hackerNews.poll_probability = hackerNews.poll_posts.shape[0] / hackerNews.total_post\n",
    "\n",
    "    model_file = open(model_file_name, \"w\", encoding=\"utf-8\")\n",
    "    line_no = 0\n",
    "    hackerNews.training_model.clear()\n",
    "\n",
    "    for word in hackerNews.vocabulary.keys():\n",
    "        line_no += 1\n",
    "        values = []\n",
    "        calculate_conditional_prob(values, word, hackerNews.story_post_vocabulary, story_post_total_words, vocabulary_size, delta)\n",
    "        calculate_conditional_prob(values, word, hackerNews.ask_post_vocabulary, ask_post_total_words, vocabulary_size, delta)\n",
    "        calculate_conditional_prob(values, word, hackerNews.show_post_vocabulary, show_post_total_words, vocabulary_size, delta)\n",
    "        calculate_conditional_prob(values, word, hackerNews.poll_post_vocabulary, poll_post_total_words, vocabulary_size, delta)\n",
    "        hackerNews.training_model[word] = values\n",
    "\n",
    "        model_file.write(create_line(line_no, word, values))\n",
    "\n",
    "    model_file.close()\n",
    "    \n",
    "    print(\"Training Model Created..!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(hackerNews, result_file_name):\n",
    "    \"\"\"\n",
    "    This method tests the testing dataset using training model, predicts the output and writes into file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hackerNews : HackerNews object\n",
    "        An object of HackerNews class.\n",
    "    result_file_name : string\n",
    "        A name of output file of testing.\n",
    "    \"\"\" \n",
    "    print(\"\\nTesting Dataset using training model....\")\n",
    "    csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "    data_2019 = csvdf[(csvdf[\"Created At\"] >= \"2019-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2019-12-31 23:59:59\")]\n",
    "    data_2019 = data_2019.groupby(\"Post Type\").head(1000)\n",
    "    \n",
    "    baseline_result = open(result_file_name, \"w\", encoding=\"utf-8\")\n",
    "    line_no = 0\n",
    "    test_post_types = []\n",
    "    predicted_post_types = []\n",
    "\n",
    "    for index, post in data_2019.iterrows():\n",
    "        line_no += 1\n",
    "        words = get_words_and_frequncy(post.Title)\n",
    "        lemmatized_words = get_lemmatized_words(words.keys())\n",
    "\n",
    "        story_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.story_probability, 1)\n",
    "        ask_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.ask_probability, 3)\n",
    "        show_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.show_probability, 5)\n",
    "        poll_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.poll_probability, 7)\n",
    "\n",
    "        predicted_post_type = predict_post_type(story_score, ask_score, show_score, poll_score)\n",
    "        original_post_type = post[\"Post Type\"]\n",
    "        output = \"right\" if original_post_type == predicted_post_type else \"wrong\"\n",
    "        values = [original_post_type, story_score, ask_score, show_score, poll_score, predicted_post_type, output]\n",
    "\n",
    "        baseline_result.write(create_line(line_no, post.Title, values))\n",
    "        test_post_types.append(original_post_type)\n",
    "        predicted_post_types.append(predicted_post_type)\n",
    "\n",
    "    baseline_result.close()\n",
    "    \n",
    "    print(\"Testing Dataset Completed..!!!\")\n",
    "    return test_post_types, predicted_post_types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome..!!!\n",
      "\n",
      "1. Create Training Model\n",
      "2. Basline Experiment\n",
      "3. Stop-word Filtering Experiment\n",
      "4. Word Length Filtering Experiment\n",
      "5. Infrequent Word Filtering Experiment\n",
      "6. Smoothing Experiment\n",
      "0. Exit\n",
      "\n",
      "Enter your choice: 0\n",
      "\n",
      "Thank You..!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome..!!!\")\n",
    "\n",
    "choice = -1\n",
    "remove_words = get_remove_words()\n",
    "remove_symbols = get_remove_symbols()\n",
    "is_stopword_exp = is_wordlength_exp = is_infrequentword_exp = False\n",
    "threshold = 0\n",
    "is_threshold_percent = False\n",
    "\n",
    "while (choice != 0):\n",
    "    DELTA = 0.5\n",
    "    accuracy_list.clear()\n",
    "    x_values.clear()\n",
    "    print(\"\\n1. Create Training Model\")\n",
    "    print(\"2. Basline Experiment\")\n",
    "    print(\"3. Stop-word Filtering Experiment\")\n",
    "    print(\"4. Word Length Filtering Experiment\")\n",
    "    print(\"5. Infrequent Word Filtering Experiment\")\n",
    "    print(\"6. Smoothing Experiment\")\n",
    "    print(\"0. Exit\")\n",
    "    choice = int(input(\"\\nEnter your choice: \"))\n",
    "    \n",
    "    if choice == 0:\n",
    "        print(\"\\nThank You..!!!\")\n",
    "        break\n",
    "    elif choice == 1:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_wordlength_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        build_vocabulary(hackerNews)\n",
    "        create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "        \n",
    "    elif choice == 2:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_wordlength_exp = is_infrequentword_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        build_vocabulary(hackerNews)\n",
    "        create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "        test_post_types, predicted_post_types = test_dataset(hackerNews, \"baseline-result.txt\")\n",
    "        check_performance(hackerNews, test_post_types, predicted_post_types)\n",
    "        \n",
    "    elif choice == 3:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = True \n",
    "        is_wordlength_exp = is_infrequentword_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        stop_words = get_stop_words()\n",
    "        build_vocabulary(hackerNews)\n",
    "        create_training_model(hackerNews, \"stopword-model.txt\", DELTA)\n",
    "        test_post_types, predicted_post_types = test_dataset(hackerNews, \"stopword-result.txt\")\n",
    "        check_performance(hackerNews, test_post_types, predicted_post_types)\n",
    "        \n",
    "    elif choice == 4:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_infrequentword_exp = False\n",
    "        is_wordlength_exp = True\n",
    "        read_training_data(hackerNews)\n",
    "        build_vocabulary(hackerNews)\n",
    "        create_training_model(hackerNews, \"wordlength-model.txt\", DELTA)\n",
    "        test_post_types, predicted_post_types = test_dataset(hackerNews, \"wordlength-result.txt\")\n",
    "        check_performance(hackerNews, test_post_types, predicted_post_types)\n",
    "        \n",
    "    elif choice == 5:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_wordlength_exp = False\n",
    "        is_infrequentword_exp = True\n",
    "        read_training_data(hackerNews)\n",
    "        \n",
    "        is_threshold_percent = False\n",
    "        thresholds = [1, 5, 10, 15, 20]\n",
    "        for i in thresholds:\n",
    "            threshold = i\n",
    "            build_vocabulary(hackerNews)\n",
    "            create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "            test_post_types, predicted_post_types = test_dataset(hackerNews, \"baseline-result.txt\")\n",
    "            check_performance(hackerNews, test_post_types, predicted_post_types)\n",
    "            x_values.append(str(len(hackerNews.vocabulary)))\n",
    "        \n",
    "        plot_performance(x_values, accuracy_list, \"Word in Vocabulary\")\n",
    "        \n",
    "        x_values.clear()\n",
    "        accuracy_list.clear()\n",
    "        is_threshold_percent = True\n",
    "        thresholds = [5, 10, 15, 20, 25]\n",
    "        for i in thresholds:\n",
    "            threshold = i\n",
    "            build_vocabulary(hackerNews)\n",
    "            create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "            test_post_types, predicted_post_types = test_dataset(hackerNews, \"baseline-result.txt\")\n",
    "            check_performance(hackerNews, test_post_types, predicted_post_types)\n",
    "            x_values.append(str(len(hackerNews.vocabulary)))\n",
    "        \n",
    "        plot_performance(x_values, accuracy_list, \"Word in Vocabulary\")\n",
    "        \n",
    "    elif choice == 6:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_wordlength_exp = is_infrequentword_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        build_vocabulary(hackerNews)\n",
    "        DELTA = 0\n",
    "        while DELTA <= 1:\n",
    "            x_values.append(str(DELTA))\n",
    "            create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "            test_post_types, predicted_post_types = test_dataset(hackerNews, \"baseline-result.txt\")\n",
    "            check_performance(hackerNews, test_post_types, predicted_post_types)\n",
    "            DELTA += 0.1\n",
    "        plot_performance(x_values, accuracy_list, \"Delta Value\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
