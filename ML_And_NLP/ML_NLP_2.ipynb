{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import bigrams\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "is_stopword_exp = False\n",
    "is_wordlength_exp = False\n",
    "remove_words = []\n",
    "stop_words = []\n",
    "\n",
    "\n",
    "class HackerNews:\n",
    "    \"\"\"\n",
    "    This class is used to hold all posts of year 2018, complete vocabulary,\n",
    "    post type wise vocabulary, post type constants, training model and pobability of each post type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.STORY = \"story\"\n",
    "        self.ASK_HN = \"ask_hn\"\n",
    "        self.SHOW_HN = \"show_hn\"\n",
    "        self.POLL = \"poll\"\n",
    "        self.total_post = 0\n",
    "        self.story_posts = []\n",
    "        self.ask_posts = []\n",
    "        self.show_posts = []\n",
    "        self.poll_posts = []\n",
    "        self.vocabulary = dict()\n",
    "        self.story_post_vocabulary = dict()\n",
    "        self.ask_post_vocabulary = dict()\n",
    "        self.show_post_vocabulary = dict()\n",
    "        self.poll_post_vocabulary = dict()\n",
    "        self.training_model = dict()\n",
    "        self.story_probability = 0.0\n",
    "        self.ask_probability = 0.0\n",
    "        self.show_probability = 0.0\n",
    "        self.poll_probability = 0.0\n",
    "\n",
    "\n",
    "\n",
    "def get_pos_tag(word) :\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def get_words_and_frequncy(sentence):\n",
    "    local_vocabulary = dict()\n",
    "    words = []\n",
    "    \n",
    "    for word in nltk.word_tokenize(sentence.lower()) :\n",
    "        word = word.replace(\"'\", \"\").strip()\n",
    "        word = word.replace(\"\\\"\", \"\").strip()\n",
    "        if len(word) == 0 :\n",
    "            continue\n",
    "        if word in remove_words :\n",
    "            continue\n",
    "        if is_stopword_exp and word in stop_words :\n",
    "            continue\n",
    "        if is_wordlength_exp and len(word) <= 2 and len(word) >= 9:\n",
    "            continue\n",
    "        add_to_vocabulary(word, local_vocabulary, 1)\n",
    "        words.append(word)\n",
    "            \n",
    "    string_bigrams = bigrams(words.copy())\n",
    "    for gram in string_bigrams: \n",
    "        word = gram[0] + \" \" + gram[1]\n",
    "        if is_wordlength_exp and len(word) <= 2 and len(word) >= 9:\n",
    "            continue\n",
    "        tag_1 = nltk.pos_tag([gram[0]])[0][1][0].upper()\n",
    "        tag_2 = nltk.pos_tag([gram[1]])[0][1][0].upper()\n",
    "        if tag_1 == \"N\" and tag_2 == \"N\":\n",
    "            add_to_vocabulary(word, local_vocabulary, 1)\n",
    "            reduce_frequency(gram[0], local_vocabulary)\n",
    "            reduce_frequency(gram[1], local_vocabulary)\n",
    "    \n",
    "    return local_vocabulary\n",
    "\n",
    "\n",
    "def add_to_vocabulary(word, vocabulary, frequency) :\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] += frequency\n",
    "    else :\n",
    "        vocabulary[word] = frequency\n",
    "\n",
    "\n",
    "def reduce_frequency(word, vocabulary):\n",
    "    if word in vocabulary :\n",
    "        vocabulary[word] -= 1\n",
    "        if vocabulary[word] == 0:\n",
    "            del vocabulary[word]\n",
    "\n",
    "\n",
    "def get_lemmatized_words(words) :\n",
    "    lemmatized_words = []\n",
    "    for word in words :\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def create_vocabulary(posts, vocabulary, post_type_vocabulary) :\n",
    "    for index, post in posts.iterrows():\n",
    "        if index > 1000:\n",
    "            break\n",
    "        local_vocabulary = get_words_and_frequncy(post.Title)\n",
    "        \n",
    "        for word, count in local_vocabulary.items():\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "            add_to_vocabulary(lemmatized_word, vocabulary, count)\n",
    "            add_to_vocabulary(lemmatized_word, post_type_vocabulary, count)\n",
    "\n",
    "\n",
    "def calculate_conditional_prob(values, word, post_type_vocab, post_type_total_words, vocabulary_size, delta):\n",
    "    word_count = 0\n",
    "    if word in post_type_vocab:\n",
    "        word_count = post_type_vocab[word]\n",
    "    conditional_prob = (word_count + delta) / (post_type_total_words + (vocabulary_size * delta))\n",
    "    values.append(word_count)\n",
    "    values.append(round(math.log10(conditional_prob),10))\n",
    "\n",
    "\n",
    "def create_line(line_no, title, values):\n",
    "    line = str(line_no) + \"  \"  + title\n",
    "    for value in values :\n",
    "        line += \"  \" + str(value)\n",
    "    line += \"\\n\"\n",
    "    return line\n",
    "\n",
    "\n",
    "def calculate_score(words, training_model, post_type_probability, index):\n",
    "    post_type_score = round(math.log10(post_type_probability),10)\n",
    "    for word in words:\n",
    "        if word in training_model:\n",
    "            post_type_score += training_model[word][index]\n",
    "    return post_type_score\n",
    "\n",
    "\n",
    "def predict_post_type(story_score, ask_score, show_score, poll_score):\n",
    "    scores = [story_score, ask_score, show_score, poll_score]\n",
    "    max_index = scores.index(max(scores))\n",
    "    if max_index == 0:\n",
    "        return hackerNews.STORY\n",
    "    elif max_index == 1:\n",
    "        return hackerNews.ASK_HN\n",
    "    elif max_index == 2:\n",
    "        return hackerNews.SHOW_HN\n",
    "    else:\n",
    "        return hackerNews.POLL\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    stop_words_file = open(\"Stopwords.txt\",\"r\")\n",
    "    stop_word_list = stop_words_file.read().split()\n",
    "    stop_words_file.close()\n",
    "    return stop_word_list\n",
    "\n",
    "\n",
    "def get_remove_words():\n",
    "    remove_words_file = open(\"remove_words.txt\",\"r\")\n",
    "    remove_word_list = remove_words_file.read().split()\n",
    "    remove_words_file.close()\n",
    "    return remove_word_list\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def read_training_data(hackerNews):\n",
    "    csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "    data_2018 = csvdf[(csvdf[\"Created At\"] >= \"2018-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2018-12-31 23:59:59\")]\n",
    "\n",
    "    hackerNews.total_post = data_2018.size\n",
    "    # print(\"Total Post: \", total_post)\n",
    "\n",
    "    data_2018 = data_2018.groupby(\"Post Type\")\n",
    "\n",
    "    hackerNews.story_posts = data_2018.get_group(hackerNews.STORY)\n",
    "    hackerNews.ask_posts = data_2018.get_group(hackerNews.ASK_HN)\n",
    "    hackerNews.show_posts = data_2018.get_group(hackerNews.SHOW_HN)\n",
    "    hackerNews.poll_posts = data_2018.get_group(hackerNews.POLL)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def create_training_model(hackerNews, model_file_name, delta):\n",
    "    # Create Vocabulary - Start\n",
    "    print(\"\\nCreating Vocabulary....\")\n",
    "    create_vocabulary(hackerNews.story_posts, hackerNews.vocabulary, hackerNews.story_post_vocabulary)\n",
    "    create_vocabulary(hackerNews.ask_posts, hackerNews.vocabulary, hackerNews.ask_post_vocabulary)\n",
    "    create_vocabulary(hackerNews.show_posts, hackerNews.vocabulary, hackerNews.show_post_vocabulary)\n",
    "    create_vocabulary(hackerNews.poll_posts, hackerNews.vocabulary, hackerNews.poll_post_vocabulary)\n",
    "    \n",
    "    # Sort vocabulary alphabetically\n",
    "    hackerNews.vocabulary = collections.OrderedDict(sorted(hackerNews.vocabulary.items(), key=lambda kv:kv[0]))\n",
    "    \n",
    "    vocabulary_file = open(\"vocabulary.txt\", \"w\")\n",
    "    for word, count in hackerNews.vocabulary.items():\n",
    "        vocabulary_file.write(word+\" \"+str(count)+\"\\n\")\n",
    "    vocabulary_file.close()\n",
    "    \n",
    "    print(\"Vocabulary Created..!!!\")\n",
    "    # Create Vocabulary - End\n",
    "    \n",
    "    # Build Training Model - Start\n",
    "    story_post_total_words = sum(hackerNews.story_post_vocabulary.values())\n",
    "    ask_post_total_words = sum(hackerNews.ask_post_vocabulary.values())\n",
    "    show_post_total_words = sum(hackerNews.show_post_vocabulary.values())\n",
    "    poll_post_total_words = sum(hackerNews.poll_post_vocabulary.values())\n",
    "    vocabulary_size = len(hackerNews.vocabulary)\n",
    "    \n",
    "    hackerNews.story_probability = hackerNews.story_posts.size / hackerNews.total_post\n",
    "    hackerNews.ask_probability = hackerNews.ask_posts.size / hackerNews.total_post\n",
    "    hackerNews.show_probability = hackerNews.show_posts.size / hackerNews.total_post\n",
    "    hackerNews.poll_probability = hackerNews.poll_posts.size / hackerNews.total_post\n",
    "\n",
    "    model_file = open(model_file_name, \"w\", encoding=\"utf-8\")\n",
    "    line_no = 0\n",
    "\n",
    "    for word in hackerNews.vocabulary.keys():\n",
    "        line_no += 1\n",
    "        values = []\n",
    "        calculate_conditional_prob(values, word, hackerNews.story_post_vocabulary, story_post_total_words, vocabulary_size, delta)\n",
    "        calculate_conditional_prob(values, word, hackerNews.ask_post_vocabulary, ask_post_total_words, vocabulary_size, delta)\n",
    "        calculate_conditional_prob(values, word, hackerNews.show_post_vocabulary, show_post_total_words, vocabulary_size, delta)\n",
    "        calculate_conditional_prob(values, word, hackerNews.poll_post_vocabulary, poll_post_total_words, vocabulary_size, delta)\n",
    "        hackerNews.training_model[word] = values\n",
    "\n",
    "        model_file.write(create_line(line_no, word, values))\n",
    "\n",
    "    model_file.close()\n",
    "    \n",
    "    print(\"Training Model Created..!!!\")\n",
    "    # Build Training Model - End\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def test_dataset(hackerNews, result_file_name):\n",
    "    # Testing dataset - Start\n",
    "    csvdf = pd.read_csv('hn2018_2019.csv', delimiter=',', encoding='utf-8')\n",
    "    data_2019 = csvdf[(csvdf[\"Created At\"] >= \"2019-01-01 00:00:00\") & (csvdf[\"Created At\"] <= \"2019-12-31 23:59:59\")]\n",
    "\n",
    "    baseline_result = open(result_file_name, \"w\", encoding=\"utf-8\")\n",
    "    line_no = 0\n",
    "\n",
    "    for index, post in data_2019.iterrows():\n",
    "        if line_no > 1000:\n",
    "            break\n",
    "        line_no += 1\n",
    "        words = get_words_and_frequncy(post.Title)\n",
    "        lemmatized_words = get_lemmatized_words(words.keys())\n",
    "\n",
    "        story_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.story_probability, 1)\n",
    "        ask_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.ask_probability, 3)\n",
    "        show_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.show_probability, 5)\n",
    "        poll_score = calculate_score(lemmatized_words, hackerNews.training_model, hackerNews.poll_probability, 7)\n",
    "\n",
    "        predicted_post_type = predict_post_type(story_score, ask_score, show_score, poll_score)\n",
    "        original_post_type = post[\"Post Type\"]\n",
    "        output = \"right\" if original_post_type == predicted_post_type else \"wrong\"\n",
    "        values = [original_post_type, story_score, ask_score, show_score, poll_score, predicted_post_type, output]\n",
    "\n",
    "        baseline_result.write(create_line(line_no, post.Title, values))\n",
    "\n",
    "\n",
    "    baseline_result.close()\n",
    "    \n",
    "    print(\"Testing Dataset Completed..!!!\")\n",
    "    # Testing dataset - End\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome..!!!\n",
      "\n",
      "1. Create Training Model\n",
      "2. Basline Experiment\n",
      "3. Stop-word Filtering Experiment\n",
      "4. Word Length Filtering Experiment\n",
      "5. Infrequent Word Filtering Experiment\n",
      "6. Smoothing Experiment\n",
      "0. Exit\n",
      "\n",
      "Enter your choice: 3\n",
      "\n",
      "Creating Vocabulary....\n",
      "Vocabulary Created..!!!\n",
      "Training Model Created..!!!\n",
      "Testing Dataset Completed..!!!\n",
      "\n",
      "1. Create Training Model\n",
      "2. Basline Experiment\n",
      "3. Stop-word Filtering Experiment\n",
      "4. Word Length Filtering Experiment\n",
      "5. Infrequent Word Filtering Experiment\n",
      "6. Smoothing Experiment\n",
      "0. Exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome..!!!\")\n",
    "\n",
    "DELTA = 0.5\n",
    "choice = -1\n",
    "remove_words = get_remove_words()\n",
    "is_stopword_exp = is_wordlength_exp = False\n",
    "\n",
    "while (choice != 0):\n",
    "    print(\"\\n1. Create Training Model\")\n",
    "    print(\"2. Basline Experiment\")\n",
    "    print(\"3. Stop-word Filtering Experiment\")\n",
    "    print(\"4. Word Length Filtering Experiment\")\n",
    "    print(\"5. Infrequent Word Filtering Experiment\")\n",
    "    print(\"6. Smoothing Experiment\")\n",
    "    print(\"0. Exit\")\n",
    "    choice = int(input(\"\\nEnter your choice: \"))\n",
    "    \n",
    "    if choice == 0:\n",
    "        print(\"\\nThank You..!!!\")\n",
    "        break\n",
    "    elif choice == 1:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_wordlength_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "        \n",
    "    elif choice == 2:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = is_wordlength_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        create_training_model(hackerNews, \"model-2018.txt\", DELTA)\n",
    "        test_dataset(hackerNews, \"baseline-result.txt\")\n",
    "        \n",
    "    elif choice == 3:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = True \n",
    "        is_wordlength_exp = False\n",
    "        read_training_data(hackerNews)\n",
    "        stop_words = get_stop_words()\n",
    "        create_training_model(hackerNews, \"stopword-model.txt\", DELTA)\n",
    "        test_dataset(hackerNews, \"stopword-result.txt\")\n",
    "        \n",
    "    elif choice == 4:\n",
    "        hackerNews = HackerNews()\n",
    "        is_stopword_exp = False\n",
    "        is_wordlength_exp = True\n",
    "        read_training_data(hackerNews)\n",
    "        create_training_model(hackerNews, \"wordlength-model.txt\", DELTA)\n",
    "        test_dataset(hackerNews, \"wordlength-result.txt.\")\n",
    "        \n",
    "    elif choice == 5:\n",
    "        is_stopword_exp = is_wordlength_exp = False\n",
    "        \n",
    "    elif choice == 6:\n",
    "        is_stopword_exp = is_wordlength_exp = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orignal Sentence: \n",
      "The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "\n",
      "New Sentence: \n",
      "The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "\n",
      "Punctuation !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "\n",
      "the tech\n",
      "tech that\n",
      "that was\n",
      "was fixed\n",
      "fixed in\n",
      "in 2018\n",
      "2018 and\n",
      "and the\n",
      "the tech\n",
      "tech that\n",
      "that still\n",
      "still needs\n",
      "needs fixing\n",
      "\n",
      "Split: The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\n",
      "V\n",
      "v\n",
      "enjoy\n",
      "N\n",
      "10.87348\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"\"\"Following mice attacks MySQL 10% HN: on UAE ASK-HR Dr. Ph.D. sagar's $300 etc. caring farmers were marching to Delhi for better living conditions. \n",
    "Delhi police on Tuesday fired water cannons and teargas shells at protesting farmers as they tried to \n",
    "break barricades with their cars, automobiles and tractors.\"\"\"\n",
    "\n",
    "sentence = \"The Tech That Was Fixed in 2018 and the Tech That Still Needs Fixing\"\n",
    "\n",
    "print(\"\\nOrignal Sentence: \")\n",
    "print(sentence)\n",
    "\n",
    "print(\"\\nNew Sentence: \")\n",
    "newSentence = sentence\n",
    "# newSentence = get_lemmatized_words(sentence)\n",
    "print(newSentence)\n",
    "print(\"\\nPunctuation\",string.punctuation)\n",
    "\n",
    "print(\"\\n\")\n",
    "string_bigrams = bigrams(nltk.word_tokenize(sentence.lower()))\n",
    "for gram in string_bigrams: \n",
    "    print(gram[0]+\" \"+gram[1])\n",
    "\n",
    "print(\"\\nSplit:\",newSentence)\n",
    "word = \"enjoyed\"\n",
    "print(nltk.pos_tag([word])[0][1][0].upper())\n",
    "print(get_pos_tag(word))\n",
    "print(lemmatizer.lemmatize(word, get_pos_tag(word)))\n",
    "# print(get_lemmatized_words(word))\n",
    "print(wordnet.NOUN.upper())\n",
    "\n",
    "print(round(10.87348434, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "\"\n",
      "'\n",
      "“\n",
      "”\n",
      "‘\n",
      "’\n",
      "#\n",
      "$\n",
      "%\n",
      "&\n",
      "(\n",
      ")\n",
      "*\n",
      "+\n",
      ",\n",
      "-\n",
      "–\n",
      ".\n",
      "/\n",
      ":\n",
      ";\n",
      "<\n",
      "=\n",
      ">\n",
      "?\n",
      "@\n",
      "[\n",
      "\\\n",
      "]\n",
      "^\n",
      "_\n",
      "`\n",
      "{\n",
      "|\n",
      "}\n",
      "~\n",
      "a\n",
      "b\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "stop_words_file = open(\"Stopwords.txt\",\"r\")\n",
    "stop_words = stop_words_file.read().split()\n",
    "stop_words_file.close()\n",
    "\n",
    "for word in stop_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
